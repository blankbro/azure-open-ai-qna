import re
from typing import (Any, List, )

from langchain.text_splitter import (TokenTextSplitter)


class CustomTextSplitter(TokenTextSplitter):

    def __init__(self, text_split_type: str = "default", **kwargs: Any):
        super().__init__(**kwargs)
        self.text_split_type = text_split_type

    def split_text(self, text: str) -> List[str]:
        if self.text_split_type == "default":
            splits = self.split_text_of_default(text)
        elif self.text_split_type == "openai":
            splits = self.split_text_of_openai(text, self._chunk_size)
        elif self.text_split_type == "houwei":
            splits = self.split_text_of_houwei(text, self._chunk_size)
        return splits

    def split_text_of_default(self, text):
        return super().split_text(text)

    def split_text_of_openai(self, text, n):
        # https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb
        # Split a text into smaller chunks of size n, preferably ending at the end of a sentence
        def create_chunks(text, n, tokenizer):
            tokens = tokenizer.encode(text)
            """Yield successive n-sized chunks from text."""
            i = 0
            while i < len(tokens):
                # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens
                j = min(i + int(1.5 * n), len(tokens))
                while j > i + int(0.5 * n):
                    # Decode the tokens and check for full stop or newline
                    chunk = tokenizer.decode(tokens[i:j])
                    if chunk.endswith(".") or chunk.endswith("。") \
                            or chunk.endswith("!") or chunk.endswith("！") \
                            or chunk.endswith("?") or chunk.endswith("？") \
                            or chunk.endswith("\n"):
                        break
                    j -= 1
                # If no end of sentence found, use n tokens as the chunk size
                if j == i + int(0.5 * n):
                    j = min(i + n, len(tokens))
                yield tokens[i:j]
                i = j

        chunks = create_chunks(text, n, self._tokenizer)
        text_chunks = [self._tokenizer.decode(chunk) for chunk in chunks]
        return text_chunks

    # https://github.com/ark338/easy_gpt_utils/blob/master/easy_gpt_utils/gpt.py
    # split text into paragraphs
    # first split text into paragraphs by '\n'
    # then split paragraphs into sentences by '.', '?', '!'
    # at last if the sentence is too long, split it by max_tokens
    # this code is mostly generated by GPT-4 and bugfixed by me
    def split_text_of_houwei(self, text, max_tokens=1800):
        paragraphs = text.split('\n')
        chunks = []
        current_chunk = ''
        current_tokens = 0

        # Returns the number of tokens in a text string
        def num_tokens_from_string(string: str) -> int:
            encoding = self._tokenizer
            num_tokens = len(encoding.encode(string))
            return num_tokens

        def split_text_by_sentence(text, max_tokens):
            sentences = re.split(r'(?<=[.!?。！？])\s*', text)
            chunks = []
            current_chunk = ''
            current_tokens = 0

            for sentence in sentences:
                sentence_tokens = num_tokens_from_string(sentence)

                if current_tokens + sentence_tokens + 1 > max_tokens:
                    chunks.append(current_chunk)
                    current_chunk = sentence
                    current_tokens = sentence_tokens
                else:
                    if current_chunk:
                        current_chunk += ' '
                    #    current_tokens += 1
                    current_chunk += sentence
                    current_tokens = num_tokens_from_string(current_chunk)

            if current_chunk:
                chunks.append(current_chunk)

            return chunks

        for paragraph in paragraphs:
            paragraph_tokens = num_tokens_from_string(paragraph)

            if paragraph_tokens > max_tokens and current_chunk == '':
                chunks.extend(split_text_by_sentence(paragraph, max_tokens))
                continue

            if current_tokens + paragraph_tokens + 1 > max_tokens:
                current_chunk += '\n'
                chunks.append(current_chunk)
                # print(f"append chunk: {current_chunk} tokens: {current_tokens}")

                if paragraph_tokens > max_tokens:
                    chunks.extend(split_text_by_sentence(paragraph, max_tokens))
                    continue
                else:
                    current_chunk = paragraph
                    current_tokens = num_tokens_from_string(paragraph)
            else:
                if current_chunk:
                    current_chunk += '\n'
                current_chunk += paragraph
                current_tokens = num_tokens_from_string(current_chunk)
                # print(f"concat paragraph: {paragraph} tokens: {current_tokens}")

        if current_chunk:
            chunks.append(current_chunk)

        return chunks


if __name__ == "__main__":
    test_cases = [
        "This is a short text. It doesn't need to be split.",
        '''1. What is AI-Based Remaining Range?
        Combining AI algorithm with the previous remaining mileage algorithm, AI-Based Remaining Range is calculated in accordance with user's riding habits, vehicle status and riding environment. We hope to provide users with more accurate remaining range along with the development of AI.
        &nbsp;
        2. What can this algorithm learn?
        Although users use the same type of vehicle in different cities, the actual range may vary due to the factors of driving habits, road conditions and vehicle status. It is our hope that, with AI-Based Remaining Range Algorithm, the remaining range displays on the app or the dashboard will be more accurate and customized for users, which may further alleviate user's mileage anxiety and help users make travel plans.
    
        1. Seasons
    
        The weather changes in four seasons have great effects on the actual range.
        As we all know, the battery level of electric products may drop quickly in winter, and the vehicle even powers off automatically. It&rsquo;s the same for eScooters with lead-acid batteries or lithium batteries. The reaction of chemical substances in the battery in a low temperature environment slows down, and the charging and discharging power will drop significantly, which will make charging time longer and the range may be not as durable as usual. When the weather gets warmer, the battery performance will get back to normal state, and the actual range will be normal.
    
        2. Driving habits
    
        For eScooters, there is a concept called Economical Speed, which means that the power consumption is minimum when you drive the vehicle at a certain constant speed. Bad driving habits such as suddenly driving at full throttle or hard braking will do bring more joy for your drive indeed, but the range will also be decreased. When we compare the physical exertio of walking with sprinting, the physical exertion caused by sprinting will lead to a greater decrease in endurance.
        In addition, the other factor is load. The range may decrease by a certain degree if the vehicle is driven by two people or carrying heavy things.
    
        3. Road condition
    
        For the AI algorithm updated this time, we took road condition into account. Besides of road condition, we also considered other factors related to road condition, tire pressure, wind resistance, etc.
        Road condition: Flat roads are the most power-saving, the range will be decreased greatly when driving in winding and bumpy gravel roads or most uphill road conditions.
        Tire pressure: Under-inflated tires will cause greater driving resistance. Compared with tires with standard tire pressure, the range can be decreased by up to 30%.
        Wind resistance: Normally, people wear more clothes in winter, compared with summer, the wind resistance is greater. The battery consumption is more obvious when you drive at a high speed.
    
        4. Future
    
        The current algorithm model vaguely contains the factors including vehicle health, tire pressure, motor efficiency, etc. In the future, we will continue the research in this respect as well as the R&amp;D related to vehicle maintenance that based on big data, so as to improve the range and maintain the vehicle condition by enabling users to fix their bad driving habits.
        Notes:
        Winter is the season with the most complex, obvious and challenging impact on the vehicle range endurance algorithm. That&rsquo;s why we choose test in winter, we hope to overcome the biggest challenges and variables in this season and provide users with more accurate mileage data.
        After tracking riding records for about two weeks or collecting enough riding mileages data, more accurate remaining mileage may be displayed. If the remaining mileage is not accurate, please record more rides or ride more mileages.
        If the actual remaining range deviates from the displayed range every time after long-time riding, please report to customer service, and our technicians will check your vehicle condition.'''
    ]

    chunk_size = 500
    chunk_overlap = 100

    myTokenTextSplitter: CustomTextSplitter = CustomTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    tokenTextSplitter: TokenTextSplitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)


    def print_result(method_name: str, chunks: List[str]):
        print(f"{method_name} len = {len(chunks)}, result = ↓↓↓")
        for chunk in chunks:
            print(f"-> {chunk}")


    for i, test_case in enumerate(test_cases):
        print(f"===> Test case {i + 1}")

        result = tokenTextSplitter.split_text(test_case)
        print_result("defaul", result)

        result = myTokenTextSplitter.split_text_of_openai(test_case, chunk_size)
        print_result("openai", result)

        result = myTokenTextSplitter.split_text_of_houwei(test_case, chunk_size)
        print_result("houwei", result)
